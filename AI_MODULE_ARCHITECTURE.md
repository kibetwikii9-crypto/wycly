# AI Module Architecture

## ðŸ“ File Location

**Recommended:** `app/services/ai.py`

**Rationale:**
- Follows existing service pattern (`telegram.py`, `processor.py`)
- Keeps all services in one directory
- Easy to import: `from app.services.ai import generate_response`
- Clear separation from business logic

**Alternative (if you prefer):**
- `app/ai/__init__.py` - Creates dedicated AI package
- `app/core/ai.py` - If you have a core module

---

## ðŸ”§ Function Signature

```python
async def generate_response(prompt: str) -> str:
    """
    Generate a text response from a prompt.
    
    Args:
        prompt: The input prompt string
        
    Returns:
        Generated text response (never None or empty string)
    """
```

**Key Characteristics:**
- âœ… **Single responsibility:** Only generates text from prompt
- âœ… **Pure function:** No side effects, no state
- âœ… **Simple interface:** One input (prompt), one output (response)
- âœ… **Type safe:** Explicit `str` return type (not `Optional[str]`)
- âœ… **Always returns:** Guaranteed non-empty string

---

## ðŸŽ¯ Why AI Must Be Abstracted Early

### 1. **Zero Business Logic in AI Module**

**Problem if AI logic is mixed:**
```python
# âŒ BAD: Business logic in AI module
async def generate_response(message, intent):
    if intent == "pricing":
        return "Our pricing is..."
    elif intent == "greeting":
        return "Hello!"
    # Business rules mixed with AI generation
```

**Solution with abstraction:**
```python
# âœ… GOOD: Pure AI function
async def generate_response(prompt: str) -> str:
    # Only generates text, no business decisions
    return llm.generate(prompt)
```

**Benefit:** Business logic stays in processor, AI module is swappable.

---

### 2. **Platform-Agnostic Design**

**Problem if platform-specific:**
```python
# âŒ BAD: Platform-specific logic
async def generate_response(telegram_message):
    # Hardcoded for Telegram
    chat_id = telegram_message.chat_id
    # ...
```

**Solution with abstraction:**
```python
# âœ… GOOD: Platform-agnostic
async def generate_response(prompt: str) -> str:
    # Works with any platform
    # Prompt is already normalized
    return llm.generate(prompt)
```

**Benefit:** Same AI module works for Telegram, WhatsApp, Instagram, etc.

---

### 3. **Easy Provider Swapping**

**Current (Placeholder):**
```python
async def generate_response(prompt: str) -> str:
    return "Placeholder response"
```

**Future (OpenAI):**
```python
async def generate_response(prompt: str) -> str:
    client = AsyncOpenAI(api_key=settings.openai_api_key)
    response = await client.chat.completions.create(...)
    return response.choices[0].message.content
```

**Future (Anthropic):**
```python
async def generate_response(prompt: str) -> str:
    client = AsyncAnthropic(api_key=settings.anthropic_api_key)
    response = await client.messages.create(...)
    return response.content[0].text
```

**Benefit:** Swap entire AI provider by changing ONE file. Zero changes to business logic.

---

### 4. **Testing & Development**

**Easy to mock:**
```python
# In tests
async def mock_generate_response(prompt: str) -> str:
    return "Mock response"

# Replace in tests
app.services.ai.generate_response = mock_generate_response
```

**Easy to develop:**
- No API keys needed during development
- No network calls during testing
- Fast iteration on business logic

---

### 5. **Cost & Performance Control**

**Centralized control:**
```python
# Easy to add rate limiting
async def generate_response(prompt: str) -> str:
    await rate_limiter.acquire()
    return await llm.generate(prompt)

# Easy to add caching
async def generate_response(prompt: str) -> str:
    cached = cache.get(prompt)
    if cached:
        return cached
    response = await llm.generate(prompt)
    cache.set(prompt, response)
    return response
```

**Benefit:** All AI-related concerns (cost, performance, caching) in one place.

---

### 6. **Error Handling Isolation**

**Centralized error handling:**
```python
async def generate_response(prompt: str) -> str:
    try:
        return await llm.generate(prompt)
    except RateLimitError:
        return "Rate limit exceeded. Please try again later."
    except APIError:
        return "AI service temporarily unavailable."
    except Exception:
        return "An error occurred. Please try again."
```

**Benefit:** All AI errors handled in one place, business logic stays clean.

---

## ðŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Business Logic Layer            â”‚
â”‚  (processor.py - Intent, Prompts)       â”‚
â”‚                                         â”‚
â”‚  - Detects intent                       â”‚
â”‚  - Builds system prompts                â”‚
â”‚  - Combines prompts                    â”‚
â”‚  - Calls AI module                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ prompt: str
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AI Abstraction Layer             â”‚
â”‚  (ai.py - Pure Function)                 â”‚
â”‚                                         â”‚
â”‚  - Takes prompt string                  â”‚
â”‚  - Returns text response                 â”‚
â”‚  - NO business logic                     â”‚
â”‚  - NO platform logic                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ Can swap implementation
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      AI Provider Implementation         â”‚
â”‚  (OpenAI / Anthropic / Local LLM)      â”‚
â”‚                                         â”‚
â”‚  - Actual LLM API calls                 â”‚
â”‚  - Provider-specific logic              â”‚
â”‚  - Error handling                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Migration Path

### Current State:
```python
# processor.py
async def generate_ai_response(message, intent):
    # Business logic + AI logic mixed
    system_prompt = build_system_prompt(intent)
    user_prompt = build_user_prompt(message)
    # Placeholder response
    return "Response..."
```

### Future State:
```python
# processor.py (business logic only)
async def generate_ai_response(message, intent):
    system_prompt = build_system_prompt(intent)
    user_prompt = build_user_prompt(message)
    full_prompt = f"{system_prompt}\n\nUser: {user_prompt}"
    
    # Delegate to AI module (pure function)
    return await ai.generate_response(full_prompt)

# ai.py (pure AI function)
async def generate_response(prompt: str) -> str:
    # Actual LLM call
    return await openai_client.chat(prompt)
```

**Migration:** Simply replace placeholder in `ai.py` with real LLM call. Zero changes to processor.

---

## âœ… Benefits Summary

1. **Separation of Concerns:** Business logic â‰  AI logic
2. **Platform Agnostic:** Works with any messaging platform
3. **Provider Agnostic:** Easy to swap OpenAI â†’ Anthropic â†’ Local LLM
4. **Testable:** Easy to mock and test
5. **Maintainable:** AI changes don't affect business logic
6. **Scalable:** Add caching, rate limiting, retries in one place
7. **Cost Control:** Centralized AI usage tracking

---

## ðŸŽ¯ Conclusion

**Early abstraction = Future flexibility**

By creating a pure AI module now, you ensure:
- âœ… Business logic stays clean
- âœ… AI provider can be swapped without refactoring
- âœ… Testing is simple
- âœ… Multi-platform support is easy
- âœ… Cost and performance control is centralized

**The AI module is a contract:** "Give me a prompt, I'll give you text."
Everything else (business rules, platform specifics, intent detection) stays in the processor.



